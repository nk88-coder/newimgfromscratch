# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTcXHzqIkQEtz7Fx5LDNkFlKmW746ZII
"""

# üß† VQ-VAE + Transformer Inference Script (standalone, GitHub-ready)

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from transformers import CLIPTokenizer, CLIPTextModel

# üì¶ Setup
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# üîß VQ-VAE Components
class VQEncoder(nn.Module):
    def __init__(self, in_channels=3, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(256, hidden_dim, 4, 2, 1), nn.ReLU(),
        )

    def forward(self, x):
        return self.encoder(x)

class VQDecoder(nn.Module):
    def __init__(self, in_channels=512):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(in_channels, 128, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1), nn.Tanh()
        )

    def forward(self, x):
        return self.decoder(x)

class EMAQuantizer(nn.Module):
    def __init__(self, num_embeddings=1024, embedding_dim=512):
        super().__init__()
        self.embedding = nn.Parameter(torch.randn(num_embeddings, embedding_dim))

    def forward(self, z):
        z = z.permute(0, 2, 3, 1).contiguous()
        z_flat = z.view(-1, z.size(-1))
        dist = (z_flat ** 2).sum(1, keepdim=True) - 2 * z_flat @ self.embedding.t() + (self.embedding ** 2).sum(1)
        indices = dist.argmin(1)
        quantized = self.embedding[indices].view(z.shape).permute(0, 3, 1, 2)
        return quantized, indices.view(z.shape[0], -1)

class VQVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = VQEncoder()
        self.quantizer = EMAQuantizer()
        self.decoder = VQDecoder()

    def forward(self, x):
        z = self.encoder(x)
        z_q, _ = self.quantizer(z)
        return self.decoder(z_q)

# üß† Transformer
class TransformerBlock(nn.Module):
    def __init__(self, dim, heads):
        super().__init__()
        self.ln1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)
        self.ln2 = nn.LayerNorm(dim)
        self.ff = nn.Sequential(
            nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim)
        )

    def forward(self, x, context):
        x = x + self.attn(self.ln1(x), context, context)[0]
        x = x + self.ff(self.ln2(x))
        return x

class AutoregressiveTransformer(nn.Module):
    def __init__(self, vocab_size=1024, dim=512, heads=4, layers=6):
        super().__init__()
        self.token_embed = nn.Embedding(vocab_size, dim)
        self.bos = nn.Parameter(torch.randn(1, 1, dim))
        self.text_proj = nn.Linear(dim, dim)
        self.layers = nn.ModuleList([TransformerBlock(dim, heads) for _ in range(layers)])
        self.out_proj = nn.Linear(dim, vocab_size)

    def forward(self, tokens, text_embed):
        B, T = tokens.shape
        x = self.token_embed(tokens)
        x = torch.cat([self.bos.expand(B, 1, -1), x[:, :-1]], dim=1)
        context = self.text_proj(text_embed).unsqueeze(1)
        for layer in self.layers:
            x = layer(x, context)
        return self.out_proj(x)

# üß™ Decode
@torch.no_grad()
def decode_tokens(vqvae, tokens):
    B, T = tokens.shape
    H = W = int(T**0.5)
    emb = vqvae.quantizer.embedding[tokens].view(B, H, W, -1).permute(0, 3, 1, 2)
    return vqvae.decoder(emb)

@torch.no_grad()
def generate_image(prompt):
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    clip = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(DEVICE)
    inputs = tokenizer([prompt], return_tensors="pt").to(DEVICE)
    text_embed = clip(**inputs).last_hidden_state.mean(1)

    checkpoint = torch.load("t2i_models.pth", map_location=DEVICE)
    vqvae = VQVAE().to(DEVICE)
    transformer = AutoregressiveTransformer().to(DEVICE)
    vqvae.load_state_dict(checkpoint["vqvae_state"])
    transformer.load_state_dict(checkpoint["transformer_state"])
    vqvae.eval(), transformer.eval()

    x = transformer.bos.expand(1, 1, -1)
    context = transformer.text_proj(text_embed).unsqueeze(1)
    tokens = []
    for _ in range(196):
        for layer in transformer.layers:
            x = layer(x, context)
        logits = transformer.out_proj(x)[:, -1]
        next_token = torch.argmax(logits, dim=-1, keepdim=True)
        tokens.append(next_token)
        next_emb = transformer.token_embed(next_token).unsqueeze(1)
        x = torch.cat([x, next_emb], dim=1)
    tokens = torch.cat(tokens, dim=1)
    img = decode_tokens(vqvae, tokens)[0].permute(1, 2, 0).cpu().numpy()
    img = (img + 1) / 2
    plt.imshow(img)
    plt.axis("off")
    plt.title(prompt)
    plt.show()

# üèÅ Entry point
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--prompt", type=str, default="a dog standing on the park")
    args = parser.parse_args()
    generate_image(args.prompt)